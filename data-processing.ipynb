{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataPath = '../input'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"setNumber = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id_col = ['id']\ncycle_col = ['cycle']\nsetting_cols = ['setting1', 'setting2', 'setting3']\nsensor_cols = ['sensor' + str(i) for i in range(1, 22)]\nrul_col = ['RUL']\nall_cols = id_col + cycle_col + setting_cols + sensor_cols + rul_col","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load train/test data with RUL","metadata":{}},{"cell_type":"code","source":"# This section is to load data\ndef loadData(fileName):\n    data = pd.read_csv(fileName, sep=\" \", header=None)\n    data.drop([26, 27], axis = 1, inplace=True)\n    data.columns = id_col + cycle_col + setting_cols +sensor_cols\n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load train RUL also returns the max cycle, and this max cycle is also the life cylce\ndef addTrainRul(data, decrease_threshold=None):\n    lifeCycles = {mcId: data[data['id']==mcId]['cycle'].max() for mcId in data['id'].unique()}\n    if decrease_threshold == None: decrease_threshold = 1\n    ruls = [lifeCycles[row[0]] - decrease_threshold if row[1] < decrease_threshold else lifeCycles[row[0]] - row[1] for row in data.values]\n    data['RUL'] = ruls\n    return lifeCycles\n    \n# use this last one only, return the data as well as the max life cycles\ndef loadTrainData(setNumber, decrease_threshold=None):\n    fileName = dataPath + '/train_FD00' + str(setNumber) + '.txt'\n    data = loadData(fileName)\n    lifeCycles = addTrainRul(data, decrease_threshold)\n    return data, lifeCycles","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decrease_threshold = None\ntrain, trainLifeCycles = loadTrainData(setNumber, decrease_threshold)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loadTestRul(fileName):\n    data = pd.read_csv(fileName, sep = \" \", header=None)\n    data.drop([1], axis=1, inplace=True)\n    data.columns = ['RUL']\n    return data\ndef addTestRul(data, rulData, decrease_threshold=None):\n    testRuls = {i+1: rulData.iloc[i, 0] for i in range(len(rulData))}\n    lifeCycles = {mcId: data[data['id']==mcId]['cycle'].max() + testRuls[mcId] for mcId in data['id'].unique()}\n    if decrease_threshold == None: decrease_threshold = 1\n    ruls = [lifeCycles[row[0]] - decrease_threshold if row[1] < decrease_threshold else lifeCycles[row[0]] - row[1] for row in data.values]\n    data['RUL'] = ruls\n    return lifeCycles\n# Use this last one only => return data as well as the max life cycles for each machine\ndef loadTestData(setNumber, decrease_threshold=None):\n    data = loadData(dataPath + '/test_FD00' +str(setNumber)+'.txt')\n    rulData = loadTestRul(dataPath + '/RUL_FD00' + str(setNumber)+'.txt')\n    lifeCycles = addTestRul(data, rulData, decrease_threshold)\n    return data, lifeCycles","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Also make test RUL becomes piecewise\ntest, testLifeCycles = loadTestData(setNumber, decrease_threshold)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot data to see its distribution","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As of feature selection they often select: 7, 8, 9, 12, 16, 17, 20  (manual selection based on sensor trends)\ndef plotSensorDataOfId(data, mcId):\n    plt.figure(figsize=(30, 20))\n    for i in range(21):\n        sensor = 'sensor'+str(i+1)\n        plt.subplot(10, 3, i+1).set_title(sensor)\n        ssdata = data[data['id']==mcId]\n        plt.plot(ssdata['cycle'], ssdata[sensor])\n    plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotSensorDataOfId(train, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotDataDistribution(data):\n    sensors = []\n    plt.figure(figsize=(30, 10))\n    for i in range(21):\n        sensor = 'sensor'+str(i+1)\n        if(len(data[sensor].unique())>1):\n            sensors.append(sensor)\n            plt.subplot(3, 10, i+1)\n            sns.distplot(data[sensor])\n    plt.tight_layout()\n    return sensors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As of feature selection they often select: 7, 8, 9, 12, 16, 17, 20 => Why 16?\ncols = plotDataDistribution(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotCorrelation(data):\n    corr = data.corr()\n    # Generate a mask for the upper triangle\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n    plt.figure(figsize=(10, 10))\n    sns.heatmap(data.corr(), square=True, mask=mask, cbar_kws={\"shrink\": 0.5})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotCorrelation(train[cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(train['sensor15'].values, train['sensor14'].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotCorrelationOfID(data, mcId):\n    data1 = data[data['id']==mcId]\n    data1 = data1.drop(['id'], axis = 1)\n    corr = data1.corr()\n    # Generate a mask for the upper triangle\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n    plt.figure(figsize=(10, 10))\n    sns.heatmap(data1.corr(), square=True, mask=mask, cbar_kws={\"shrink\": 0.5})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotCorrelationOfID(train[['id']+cols], 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scale","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale the data and return the scaled data in form of a df and the scaler (will generate the scaler if doesn't pass it)\ndef scaleData(data, scaler=None):\n    scaled_fields = setting_cols+sensor_cols\n    if scaler == None:\n        scaler = StandardScaler().fit(data[scaled_fields].values)\n#         scaler = MinMaxScaler().fit(data[scaled_fields].values)\n    scaled_data = scaler.transform(data[scaled_fields].values)\n    scaled_df0 = pd.DataFrame(scaled_data)\n    scaled_df0.columns = scaled_fields\n    scaled_df1 = data.copy()\n    for i in range(len(scaled_fields)):\n        theField = scaled_fields[i]\n        scaled_df1[theField] = scaled_df0[theField]\n    return scaled_df1, scaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaled train\nscaled_train, scaler = scaleData(train)\n# Scaled test\nscaled_test, scaler = scaleData(test, scaler)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot to check the distribution are still the same after scaling","metadata":{}},{"cell_type":"code","source":"# plot to make sure that the scaled data still keep its shape.\ncols = plotDataDistribution(scaled_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot to see if the data keeps its distribution\ncols = plotDataDistribution(scaled_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotSensorDataOfId(scaled_train, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Piece-wise data (all columns in order) with augmentation and padded sequence","metadata":{}},{"cell_type":"code","source":"import random\ndef getPieceWiseData(data, augmentStartCycle=None, augmentEndCycle=None, movingAverage=None):\n    uniqueIds = data['id'].unique()\n    if movingAverage==None:\n        result = [data[data['id']==mId].values for mId in uniqueIds]\n    else:\n        result = [data[data['id']==mId].rolling(movingAverage).mean().dropna().values for mId in uniqueIds]\n    maxlen = np.max([len(x) for x in result])\n    #Augment the data now\n    if(augmentStartCycle!=None and augmentEndCycle!= None):\n        result1 = []\n        for mc in result:\n            maxCycle = len(mc)\n            for i in range(50):\n                idx = random.randint(max([maxCycle-145, 10]), max([maxCycle-10, 10]))\n                if(len(mc[:idx, :])>0):\n                    result1.append(mc[:idx, :])\n            #Also add the complete sequence.\n#             result1.append(mc)\n        result = result1\n    # calculate the ruls (-1) is the last column for RUL\n    ruls = [min(mc[:, -1]) for mc in result]\n    return result,ruls, maxlen\n# Use this last one only (prev one is a helper)\nfrom keras.preprocessing.sequence import pad_sequences\ndef getPaddedSequence(data, pad_type='pre', maxlen=None, augmentStartCycle=None, augmentEndCycle=None, movingAverage=None):\n    piece_wise, ruls, ml = getPieceWiseData(data, augmentStartCycle, augmentEndCycle, movingAverage)\n    if(maxlen==None): maxlen = ml\n    padded_sequence = pad_sequences(piece_wise, padding=pad_type, maxlen=maxlen, dtype='float32')\n    return padded_sequence, ruls, maxlen","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augmentStartCycle = 130\naugmentEndCycle = 362\nmaxlen=200\nmovingAverage = None\npadded_train, train_ruls, maxlen = getPaddedSequence(scaled_train, maxlen=maxlen, augmentStartCycle=augmentStartCycle, augmentEndCycle=augmentEndCycle, movingAverage=movingAverage)\npadded_test, test_ruls, maxlen = getPaddedSequence(scaled_test, maxlen=maxlen, movingAverage=movingAverage)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(train_ruls)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot data to see if it is smoothened.","metadata":{}},{"cell_type":"code","source":"def plotDataForIndex(data, theIndex):\n    plt.figure(figsize=(30, 30))\n    for i in range(5, 26):\n        plt.subplot(7, 3, i-4)\n        values = data[theIndex][:, i]\n        plt.plot(range(len(values)) ,values)\n        plt.title('sensor'+str(i-4))\n        plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotDataForIndex(padded_train, 450)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Select sensors","metadata":{}},{"cell_type":"code","source":"# selected_sensors = [7, 8, 9, 12, 16, 17, 20]\nselected_sensors = [2, 3, 4, 6, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21]\n# selected_sensors = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\nselected_sensors_indices = [x-1 for x in selected_sensors] # -1 because the index starts from 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train test data","metadata":{}},{"cell_type":"code","source":"# X_train = padded_train[:, :, 5:26]\nX_train = padded_train[:, :, 5:26][:, :, selected_sensors_indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_test = padded_test[:, :, 5:26]\nX_test = padded_test[:, :, 5:26][:, :, selected_sensors_indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = np.array(train_ruls).reshape(-1,1)\ny_test = np.array(test_ruls).reshape(-1,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numOfSensors = len(X_train[0][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exporting and downloading data","metadata":{}},{"cell_type":"code","source":"import codecs, json\ndef exportNPArrayToJSON(a, fileName):\n    b = a.tolist() # nested lists with same data, indices\n    json.dump(b, codecs.open(fileName, 'w', encoding='utf-8')) ### this saves the array in .json format","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_FD = 'test_FD00' + str(setNumber) + \".json\"\ntrain_FD = 'train_FD00' + str(setNumber) + \".json\"\ntest_RUL_FD = 'test_RUL_FD00' + str(setNumber) + \".json\"\ntrain_RUL_FD = 'train_RUL_FD00' + str(setNumber) + \".json\"\n# exportNPArrayToJSON(X_train, train_FD)\nexportNPArrayToJSON(X_test, test_FD)\n# exportNPArrayToJSON(y_train, train_RUL_FD)\nexportNPArrayToJSON(y_test, test_RUL_FD)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink(test_FD)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink(train_FD)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink(test_RUL_FD)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink(train_RUL_FD)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM Model","metadata":{}},{"cell_type":"code","source":"from keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Activation\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\n\n# from keras import backend as K\n# K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=36, inter_op_parallelism_threads=36)))\n\n\ndef createModel(l1Nodes, l2Nodes, d1Nodes, d2Nodes, inputShape):\n    # input layer\n    lstm1 = LSTM(l1Nodes, input_shape=inputShape, return_sequences=True, kernel_regularizer=regularizers.l2(0.1))\n    do1 = Dropout(0.2)\n    \n    lstm2 = LSTM(l2Nodes, return_sequences=True, kernel_regularizer=regularizers.l2(0.1))\n    do2 = Dropout(0.2)\n    \n    flatten = Flatten()\n    \n    dense1 = Dense(d1Nodes, activation='relu', kernel_regularizer=regularizers.l2(0.1))\n    do3 = Dropout(0.2)\n    \n    dense2 = Dense(d2Nodes, activation='relu', kernel_regularizer=regularizers.l2(0.1))\n    do4 = Dropout(0.2)\n    \n    # output layer\n    outL = Dense(1, activation='relu', kernel_regularizer=regularizers.l2(0.1))\n    # combine the layers\n#     layers = [lstm1, do1, lstm2, do2, dense1, do3, dense2, do4, outL]\n    layers = [lstm1, lstm2, do2, flatten,  dense1, dense2, outL]\n    # create the model\n    model = Sequential(layers)\n    model.compile(optimizer='adam', loss='mse')\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = createModel(64, 64, 64, 8, (maxlen, numOfSensors))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN + LSTM","metadata":{"trusted":true}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Convolution1D\nfrom keras.layers import MaxPooling1D\nfrom keras.layers import Flatten\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Activation\nfrom keras.layers import Dropout","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def createCNNLSTMModel(inputShape):\n    cv1 = Convolution1D(input_shape=inputShape, filters=18, kernel_size=2, strides=1, padding='same', activation='relu', name='cv1')\n    mp1 = MaxPooling1D(pool_size=2, strides=2, padding='same', name = 'mp1')\n    \n    cv2 = Convolution1D(filters=36, kernel_size=2, strides=1, padding='same', activation='relu', name='cv2')\n    mp2 = MaxPooling1D(pool_size=2, strides=2, padding='same', name= 'mp2')\n    \n    cv3 = Convolution1D(filters=72, kernel_size=2, strides=1, padding='same', activation='relu', name='cv3')\n    mp3 = MaxPooling1D(pool_size=2, strides=2, padding='same', name= 'mp3')\n    \n    d4 = Dense(inputShape[0]*inputShape[1], activation='relu')\n    do4 = Dropout(0.2)\n    \n    lstm5 = LSTM(inputShape[1]*3, return_sequences=True)\n    do5 = Dropout(0.2)\n    \n    lstm6 = LSTM(inputShape[1]*3)\n    do6 = Dropout(0.2)\n    \n    d7 = Dense(50, activation='relu')\n    do7 = Dropout(0.2)\n    \n    dout = Dense(1)\n    \n    model = Sequential([cv1, mp1, cv2, mp2, cv3, mp3, d4, do4, lstm5, do5, lstm6, do6, d7, do7, dout])\n    model.compile(optimizer='rmsprop', loss='mse')\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = createCNNLSTMModel((maxlen, numOfSensors))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Average model","metadata":{}},{"cell_type":"code","source":"# from keras.models import Sequential\n# from keras.layers import Convolution1D\n# from keras.layers import MaxPooling1D\n# from keras.layers import AveragePooling1D\n# from keras.layers import Flatten\n# from keras.layers import LSTM\n# from keras.layers import Dense\n# from keras.layers import Activation\n# from keras.layers import Dropout","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def createAVGLSTMModel(inputShape):\n#     a1 = AveragePooling1D(pool_size=5,stride=1, padding='same', input_shape=inputShape)  \n# #     d4 = Dense(inputShape[0]*inputShape[1], activation='relu')\n# #     do4 = Dropout(0.5)\n    \n#     lstm5 = LSTM(64, return_sequences=True)\n#     do5 = Dropout(0.5)\n    \n#     lstm6 = LSTM(64)\n#     do6 = Dropout(0.5)\n    \n#     d7 = Dense(8, activation='relu')\n#     do7 = Dropout(0.5)\n    \n#     d8 = Dense(8, activation='relu')\n#     do8 = Dropout(0.5)\n\n    \n#     dout = Dense(1)\n    \n#     model = Sequential([a1, lstm5, do5, lstm6, do6, d7, do7, d8, do8, dout])\n#     model.compile(optimizer='adam', loss='mse')\n#     return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = createAVGLSTMModel((maxlen, numOfSensors))\n# model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train/Fit model","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import load_model\n# ten fold\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=3, shuffle=True)\nfrom keras.models import load_model\nmsescores = []\ncounter= 0\nfor trainIdx, testIdx in kfold.split(X_train, y_train):\n    counter = counter + 1\n    # create callbacks\n    model_path = 'best_model_set'+str(setNumber)+'fold'+str(counter)+'.h5'\n    mc = ModelCheckpoint(model_path, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1)\n    # create model\n    # model = createModel(64, 64, 8, 8, (maxlen, numOfSensors))\n    model = createCNNLSTMModel((maxlen, numOfSensors))\n    model.fit(X_train[trainIdx], y_train[trainIdx], validation_data=(X_train[testIdx], y_train[testIdx]), batch_size=32, epochs=4, callbacks=[mc, es])\n    # Done load the best model of this fold\n    saved_model = load_model(model_path)\n    msescores.append({'path': model_path, 'mse': saved_model.evaluate(X_train[testIdx], y_train[testIdx])})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msescores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor md in msescores:\n    saved_model = load_model(md['path'])\n    print(saved_model.evaluate(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = saved_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(50, 10))\nplt.plot(range(len(predicted)), predicted, '-x', label='predicted')\nplt.plot(range(len(y_test)), y_test, '-o', label='actual')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize intermediate outputs","metadata":{"trusted":true}},{"cell_type":"code","source":"from keras.models import Model\ndef getVizModel(model):\n    output_layers = [l.output for l in model.layers]\n    viz_model = Model(saved_model.input, output_layers)\n    return viz_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"viz_model = getVizModel(saved_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layer_outputs = viz_model.predict(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layer_outputs[0].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\ndef plotLayerData(layer_data, mcIndex):\n    mcData = layer_data[mcIndex]\n    plt.figure(figsize=(30, 30))\n    nCols = 2\n    nRows = math.ceil(len(mcData[0])/nCols)\n    for i in range(len(mcData[0])):\n        plt.subplot(nRows, nCols, i+1)\n        plt.plot(range(len(mcData[:, i])), mcData[:, i])\n        plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotLayerData(layer_outputs[1], 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\ndef plotLayerHeatmap(layer_data, mcIndex):\n    mcData = layer_data[mcIndex]\n    plt.figure(figsize=(30, 10))\n    sns.heatmap(mcData.transpose())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotSensorDataOfId(train, 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotLayerHeatmap(layer_outputs[1], 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}